{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"VVCYqVQsFX3H"},"outputs":[],"source":["from sklearn.metrics import recall_score, precision_score, f1_score, roc_auc_score,accuracy_score\n","from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.tree import DecisionTreeClassifier\n","from multiprocessing.pool import ThreadPool\n","from sklearn.naive_bayes import GaussianNB\n","from collections import deque, defaultdict\n","from imblearn.over_sampling import SMOTE\n","from warnings import filterwarnings\n","from xgboost import XGBClassifier\n","from scipy.special import softmax\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","from keras import backend as K\n","from typing import Callable\n","import tensorflow as tf\n","from sklearn import svm\n","from typing import List\n","import seaborn as sns\n","import pandas as pd\n","import numpy as np\n","import requests\n","import pickle\n","import random\n","import keras\n","import copy\n","import json\n","import sys\n","import os\n","import gc\n","\n","sns.set(rc = {'figure.figsize':(22,12)}, style=\"whitegrid\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33581,"status":"ok","timestamp":1690696137140,"user":{"displayName":"Fatrmah rac","userId":"03011577188967940535"},"user_tz":-180},"id":"5Wy2RLh8SiIz","outputId":"dc12a552-d96e-4c8d-c6e8-78634f836dc5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xTZuJfofSesG"},"outputs":[],"source":["data_path = '/content/drive/My Drive/Colab Notebooks/Muawiya/Genetic Programming Combiner with DFS/data'\n","code_path = '/content/drive/My Drive/Colab Notebooks/Muawiya/Genetic Programming Combiner with DFS/codes'\n","results_path = '/content/drive/My Drive/Colab Notebooks/Muawiya/Genetic Programming Combiner with DFS/results'\n","feature_selection_results = '/content/drive/My Drive/Colab Notebooks/Muawiya/Genetic Programming Combiner with DFS/feature_selection_results'\n","feature_selection_results_evolving = '/content/drive/My Drive/Colab Notebooks/Muawiya/Genetic Programming Combiner with DFS/feature_selection_results_evolving'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"leRBlofFVGLw"},"outputs":[],"source":["sys.path.insert(0,code_path)\n","from genetic_programming import SymbolicRegressor\n","from binirizer import CustomLabelBinirizer\n","from ensemble import Ensemble, Classifier\n","from oselm import OSELMClassifier,set_use_know\n","from DynamicFeatureSelection import dynamic_feature_selection"]},{"cell_type":"markdown","metadata":{"id":"8Pd-RhXiqz7f"},"source":["# 1. Policy Function & DQN Architicture\n","* 1- epsilon greedy implementaion for make action\n","* 2- DQN model and compile"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OwP_3Tj_FXz0"},"outputs":[],"source":["def epsilon_greedy(expected_reward, epsilon=0.97) -> int:\n","    \"\"\"\n","    expected_reward: list of expected rewards for each possible action\n","    epsilon: .\n","    \"\"\"\n","    if np.random.rand() <= epsilon:\n","        return np.random.choice(list(range(len(expected_reward))))\n","    else:\n","        return np.argmax(expected_reward)\n","\n","PolicyFunction  = Callable[[np.ndarray, float], int]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FCL570_PFXxS"},"outputs":[],"source":["LEARNING_RATE = 0.001\n","\n","def create_model(input_dim):\n","    K.clear_session()\n","    model = keras.models.Sequential()\n","    model.add(keras.layers.Input(shape=(input_dim,)))\n","    model.add(keras.layers.Dense(32, kernel_initializer='he_uniform', activation='relu'))\n","    model.add(keras.layers.Dense(16, kernel_initializer='he_uniform', activation='relu'))\n","    model.add(keras.layers.Dense(2))\n","    model.compile(loss='mse', optimizer='adam')\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"2Y2BCuRD7rZE"},"source":["## 2. Agents Implementaion\n","\n","* Agents class will be the parent of 4 types of agents listed as follow:\n","    * 1- Softmax version (distrbute the total reward between agents using softmax function)\n","    * 2- Average version (distrbute the total reward between agents using average function)\n","    * 1- regression version (calcualte the contrbution of each agent using regression model)\n","    * 1- Single Agent version (only one agent at a time can make action and get the total reawrd as a result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JgYQ8fao3YFJ"},"outputs":[],"source":["class Agents:\n","    def __init__(self, evaluation_network,number_of_featuer, buffer_size: int = 800):\n","\n","        self.evaluation_network = evaluation_network\n","        self.target_network = copy.deepcopy(self.evaluation_network)\n","        self.buffer_size = buffer_size\n","        self.fitted = False\n","        self.number_of_featuer = number_of_featuer\n","        # reply buffer is a list of tuples each tuples contains the following\n","        # (St, At, St+1, Rt+1)\n","        # (Current state, Action was made, New state, Reward)\n","        self.reply_buffer = deque(maxlen=self.buffer_size)\n","        self.contrbution = np.random.rand()\n","\n","    def make_action(self, curr_state: np.ndarray, policy_function: PolicyFunction, epsilon) -> int:\n","        # q_values represents the expected rewards for each possible action\n","        if self.fitted:\n","            q_values = self.evaluation_network.predict(curr_state.reshape(-1, self.number_of_featuer),verbose=0)\n","            action = policy_function(q_values, epsilon)\n","        else:\n","            action = policy_function([0, 1], 1)\n","        return action\n","\n","    def update_target_network(self):\n","        self.target_network = copy.deepcopy(self.evaluation_network)\n","        return"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XQSWchnH7oA8"},"outputs":[],"source":["class AgentsSoftmax(Agents):\n","    # class variable\n","    agent_count = 0\n","    def __init__(self, evaluation_network,number_of_featuer, buffer_size=800):\n","        self.agent_id = AgentsSoftmax.agent_count\n","        AgentsSoftmax.agent_count += 1\n","        super().__init__(evaluation_network,number_of_featuer, buffer_size)\n","\n","\n","    def update_evaluation_network(self, batch_size=32, epochs=5, discount_factor=0.995):\n","        # select random batch from the reply buffer\n","        batch = random.sample(self.reply_buffer, batch_size)\n","\n","        # inintilize some lists to store transition information\n","        Q1, actions, Q2, rewards = [], [], [], []\n","\n","        # from each transition extract its values\n","        for transition in batch:\n","            Q1.append(transition[0])\n","            actions.append(transition[1])\n","            Q2.append(transition[2])\n","            rewards.append(transition[3])\n","        # X_train will be the states from\n","        X_train = np.array(Q1).reshape(-1, self.number_of_featuer)\n","\n","        expected_reward = self.evaluation_network.predict(np.array(Q1).reshape(-1, self.number_of_featuer),verbose=0)\n","        Q2 = self.target_network.predict(np.array(Q2).reshape(-1, self.number_of_featuer),verbose=0)\n","\n","        # update expected rewards using biliman equation\n","\n","        for i, act in enumerate(actions[:-1]):\n","            expected_reward[i, act] = rewards[i] + (discount_factor * np.argmax(Q2[i]))\n","\n","        y_train = expected_reward.copy()\n","\n","        # calculate the change frequency of the agent decision to use it as its contrbution in get total reward\n","\n","        change_frequency = 0\n","        for state, next_state, reward, next_reward in zip(X_train[:-1], X_train[1:], rewards[: -1], rewards[1:]):\n","            #print(state, next_state, reward, next_reward)\n","            if np.abs(state[self.agent_id] - next_state[self.agent_id]) == 1:\n","                self.contrbution += np.abs(reward - next_reward)\n","                change_frequency += 1\n","\n","        self.contrbution = 0 if change_frequency==0 else self.contrbution/change_frequency\n","\n","        # train the DQN evaluation network.\n","        self.evaluation_network.fit(X_train, y_train, epochs=epochs, verbose=0)\n","        self.fitted = True\n","        return"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fn_yLK3h3YFK"},"outputs":[],"source":["class AgentsRegression(Agents):\n","    # class variable\n","    agent_count = 0\n","    def __init__(self, evaluation_network,number_of_featuer, buffer_size=800):\n","        self.agent_id = AgentsRegression.agent_count\n","        AgentsRegression.agent_count += 1\n","        super().__init__(evaluation_network,number_of_featuer, buffer_size)\n","\n","    def update_evaluation_network(self, batch_size=32, epochs=5, discount_factor=0.995):\n","        # select random batch from the reply buffer\n","        batch = random.sample(self.reply_buffer, batch_size)\n","\n","        # inintilize some lists to store transition information\n","        Q1, actions, Q2, rewards = [], [], [], []\n","\n","        # from each transition extract its values\n","        for transition in batch:\n","            Q1.append(transition[0])\n","            actions.append(transition[1])\n","            Q2.append(transition[2])\n","            rewards.append(transition[3])\n","        # X_train will be the states from\n","        X_train = np.array(Q1).reshape(-1, self.number_of_featuer)\n","\n","        expected_reward = self.evaluation_network.predict(np.array(Q1).reshape(-1, self.number_of_featuer),verbose=0)\n","        Q2 = self.target_network.predict(np.array(Q2).reshape(-1, self.number_of_featuer),verbose=0)\n","\n","        # update expected rewards using biliman equation\n","        for i, act in enumerate(actions[:-1]):\n","            expected_reward[i, act] = rewards[i] + (discount_factor * np.argmax(Q2[i]))\n","\n","        y_train = expected_reward.copy()\n","        # train the DQN evaluation network.\n","        self.evaluation_network.fit(X_train, y_train, epochs=epochs, verbose=0)\n","        self.fitted = True\n","        return"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3BzhumpF3YFL"},"outputs":[],"source":["class AgentsAverage(Agents):\n","    # class variable\n","    agent_count = 0\n","    def __init__(self, evaluation_network,number_of_featuer, buffer_size=800):\n","        self.agent_id = AgentsAverage.agent_count\n","        AgentsAverage.agent_count += 1\n","        super().__init__(evaluation_network,number_of_featuer, buffer_size)\n","\n","    def update_evaluation_network(self, batch_size=32, epochs=5, discount_factor=0.995):\n","        # select random batch from the reply buffer\n","        batch = random.sample(self.reply_buffer, batch_size)\n","\n","        # inintilize some lists to store transition information\n","        Q1, actions, Q2, rewards = [], [], [], []\n","\n","        # from each transition extract its values\n","        for transition in batch:\n","            Q1.append(transition[0])\n","            actions.append(transition[1])\n","            Q2.append(transition[2])\n","            rewards.append(transition[3])\n","        # X_train will be the states from\n","        X_train = np.array(Q1).reshape(-1, self.number_of_featuer)\n","\n","        expected_reward = self.evaluation_network.predict(np.array(Q1).reshape(-1, self.number_of_featuer),verbose=0)\n","        Q2 = self.target_network.predict(np.array(Q2).reshape(-1, self.number_of_featuer),verbose=0)\n","\n","        # update expected rewards using biliman equation\n","\n","        for i, act in enumerate(actions[:-1]):\n","            expected_reward[i, act] = rewards[i] + (discount_factor * np.argmax(Q2[i]))\n","\n","        y_train = expected_reward.copy()\n","\n","        WINDOW_SIZE = 4\n","        X_train_ = np.zeros((X_train.shape[0] // WINDOW_SIZE, X_train.shape[1]))\n","        y_train_ = []\n","        j = 0\n","        for i in range(0, batch_size, WINDOW_SIZE):\n","            window_of_states = X_train[i: i + WINDOW_SIZE].sum(axis=0) / WINDOW_SIZE\n","            window_of_rewards = sum(rewards[i: i + WINDOW_SIZE])\n","            r = window_of_rewards * window_of_states[self.agent_id]\n","             # Rounding state\n","            X_train_[j, :] = np.around(window_of_states)\n","            if window_of_states[self.agent_id] == 0:\n","                if window_of_rewards > 0.6:\n","                    r = window_of_rewards\n","                else:\n","                    r = window_of_rewards / WINDOW_SIZE\n","            y_train_.append(r)\n","            j += 1\n","\n","        X_train = X_train_\n","        y_train = np.array(y_train_)\n","        # train the DQN evaluation network.\n","        self.evaluation_network.fit(X_train, y_train, epochs=epochs, verbose=0)\n","        self.fitted = True\n","        return"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H0rnKcjH3YFM"},"outputs":[],"source":["class AgentsSingle(Agents):\n","    # class variable\n","    agent_count = 0\n","    def __init__(self, evaluation_network,number_of_featuer, buffer_size=800):\n","        self.agent_id = AgentsSingle.agent_count\n","        AgentsSingle.agent_count += 1\n","        super().__init__(evaluation_network,number_of_featuer, buffer_size)\n","\n","    def update_evaluation_network(self, batch_size=32, epochs=5, discount_factor=0.995):\n","        # select random batch from the reply buffer\n","        batch = random.sample(self.reply_buffer, batch_size)\n","\n","        # inintilize some lists to store transition information\n","        Q1, actions, Q2, rewards = [], [], [], []\n","\n","        # from each transition extract its values\n","        for transition in batch:\n","            Q1.append(transition[0])\n","            actions.append(transition[1])\n","            Q2.append(transition[2])\n","            rewards.append(transition[3])\n","        # X_train will be the states from\n","        X_train = np.array(Q1).reshape(-1, self.number_of_featuer)\n","\n","        expected_reward = self.evaluation_network.predict(np.array(Q1).reshape(-1, self.number_of_featuer),verbose=0)\n","        Q2 = self.target_network.predict(np.array(Q2).reshape(-1, self.number_of_featuer),verbose=0)\n","\n","        # update expected rewards using biliman equation\n","        for i, act in enumerate(actions[:-1]):\n","            expected_reward[i, act] = rewards[i] + (discount_factor * np.argmax(Q2[i]))\n","\n","        y_train = expected_reward.copy()\n","        # train the DQN evaluation network.\n","        self.evaluation_network.fit(X_train, y_train, epochs=epochs, verbose=0)\n","        self.fitted = True\n","        return"]},{"cell_type":"markdown","metadata":{"id":"h7WClTTW3YFM"},"source":["# 3. Reward Calculation Method\n","* 1- get accuracy of selected feature using logistic regression model\n","* 2- claculate the reward with reward_strategy function using accuracy from last step."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2k2omtZ2TV12"},"outputs":[],"source":["class Classifier:\n","    def __init__(self, clf, max_number_of_classes:int=2):\n","        \"\"\"\n","        Wrapping sklearn classifiers\n","        clf: sklearn classifiers like (KNN, LogRegression, DecisionTree, etc...)\n","        max_number_of_classes: integer, number of unique values in the predicted variable.\n","        \"\"\"\n","        self.clf = clf\n","        # decision profile contains the prediction probability values.\n","        self.decision_profile = None\n","        self.max_number_of_classes = max_number_of_classes\n","\n","\n","    # fit the classifier\n","    def fit(self, X_train, y_train, unselected_features=None):\n","        \"\"\"\n","        Call the training function\n","        X_train: 2d array with shape num_of_samples x num_of_feautres.\n","        y_train: 1d array with shape (num_of_samples, ) contains the ground truth values.\n","        \"\"\"\n","        # X_train = np.array(X_train) if not type(X_train).__module__ == np.__name__ else X_train\n","        # y_train = np.array(y_train) if not type(y_train).__module__ == np.__name__ else y_train\n","\n","\n","        if type(self.clf) == OSELMClassifier:\n","            self.clf.fit(X_train, y_train, unselected_features)\n","        else:\n","            # print(\"1234\")\n","            self.clf.fit(X_train, y_train)\n","            # print(type(self.clf))\n","\n","    def predict_proba(self, X):\n","        \"\"\"\n","        predict the probability of belonging this `sample` to each class\n","        \"\"\"\n","        # sometimes number of unique values in the predicted variable differ from one chunk to another,\n","        # so that we need to pad the results of probablity prediction to new size equal to `max_number_of_classes`\n","\n","        pred = self.clf.predict_proba(X)\n","        return pred\n","\n","    def build_decision_profile(self, sample):\n","        \"\"\"\n","        add the predict_probability result to the `decision_profile` list\n","        sample: one example form the dataset\n","        \"\"\"\n","        self.decision_profile = self.predict_proba(sample.reshape((1, -1)))[0].tolist()\n","\n","\n","class Ensemble:\n","    def __init__(self, classifiers, program, apply_model_replacement):\n","\n","        \"\"\"\n","        classfiers : list of Classifier objects\n","        program: result of genetic programming (SymbolicRegressor)\n","        \"\"\"\n","        self.classifiers = classifiers\n","        self.program = program\n","        self.program_history = []\n","        self.fitted = False\n","        self.scores = {}\n","        self.apply_model_replacement = apply_model_replacement\n","\n","    def fit(self, X_train, y_train, unselected_features=None):\n","        self.classifier_induction(self.classifiers, X_train, y_train, unselected_features=unselected_features)\n","        self.update_program(X_train, y_train)\n","\n","\n","    def classifier_induction(self, new_classifiers, X_train:np.array, y_train:np.array, unselected_features:list=None)->list:\n","        \"\"\"\n","        new_classifiers: list of new classifiers to insert them into ensemble classifiers.\n","        X_train: training dataset .\n","        y_train: ground truth values.\n","        unselected_features: indices of unselected features at each chunk\n","        ----------------------------------------------------------------\n","        return new_classifiers after training.\n","        \"\"\"\n","        # use classifier_induction_util for multiprocessing\n","        def classifier_induction_util(classifier):\n","            clf = Classifier(classifier, 2)\n","            clf.fit(X_train.copy(), y_train.copy(), unselected_features)\n","            return clf\n","        # train each new classifier in parallel\n","        trained_classifiers = ThreadPool(len(new_classifiers)).map(classifier_induction_util, new_classifiers)\n","        # add the trained classifiers to the ensemble classifiers.\n","        if self.apply_model_replacement:\n","          self.classifiers += trained_classifiers\n","        else:\n","          self.classifiers = trained_classifiers\n","        # return the trained classifiers (new classifiers after training)\n","        return trained_classifiers\n","\n","    def model_replacement(self, criteria='best'):\n","        if criteria == 'best':\n","          pass\n","        elif criteria == 'time':\n","          self.classifiers = self.classifiers[3:]\n","\n","\n","    def global_support_degree(self, sample):\n","        for i,clf in enumerate(self.classifiers):\n","            if not isinstance(clf,Classifier):\n","              clf = Classifier(clf,2)\n","              self.classifiers[i] = clf\n","            clf.build_decision_profile(sample)\n","        profile = np.array([self.classifiers[i].decision_profile for i in range(len(self.classifiers))])\n","        return np.argmax(profile.sum(axis=0))\n","\n","    def update_program(self, X, y):\n","        # change the fit flag to True.\n","        self.fitted = True\n","        profiles = np.array([self.classifiers[i].predict_proba(X) for i in range(len(self.classifiers))])\n","        self.program.fit(profiles, y)\n","        self.program_history.append(self.program)\n","\n","\n","    def predict(self, X_test):\n","        X_test = np.squeeze(X_test) if len(list(X_test.shape))>2 else X_test\n","        profiles = np.array([self.classifiers[i].predict_proba(X_test) for i in range(len(self.classifiers))])\n","        return self.program.predict(profiles)\n","\n","    def evaluate(self, X_test, y_test, chunk_id=1):\n","        y_pred = self.predict(X_test)\n","        # accuracy_score, precision_score, recall_score, f1_score\n","        try:\n","          auc = roc_auc_score(y_test, y_pred)\n","        except:\n","          auc = 0.5\n","        self.scores[chunk_id] = {\"accuracy\": accuracy_score(y_test, y_pred),\n","                                 \"precision\": precision_score(y_test, y_pred),\n","                                 \"recall\": recall_score(y_test, y_pred),\n","                                 \"f1-score\": f1_score(y_test, y_pred),\n","                                 \"auc\": auc}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-4Uoh7nJVoFA"},"outputs":[],"source":["def genetic_programming():\n","    return SymbolicRegressor(population_size=10,\n","            generations=5, stopping_criteria=0.85,\n","            p_crossover=0.7, p_subtree_mutation=0.1,\n","            p_hoist_mutation=0.05, p_point_mutation=0.1,\n","            max_samples=0.7, verbose=1,\n","            parsimony_coefficient=1e-4, random_state=42,\n","            function_set=['avg2', 'avg3', 'avg5',\n","                          'median3', 'median5', 'maximum2', 'maximum3', 'maximum5'],\n","            metric='f1-score')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7qPhNaatTSLY"},"outputs":[],"source":["def generate_oselm_models(number_of_hidden_neurons, apply_model_replacement=False):\n","    models= [OSELMClassifier(number_of_hidden_neurons, 'relu', binarizer=CustomLabelBinirizer(), random_state=42),\n","             OSELMClassifier(number_of_hidden_neurons, 'relu', binarizer=CustomLabelBinirizer(), random_state=42),\n","             OSELMClassifier(number_of_hidden_neurons, 'relu', binarizer=CustomLabelBinirizer(), random_state=42),\n","             OSELMClassifier(number_of_hidden_neurons, 'relu', binarizer=CustomLabelBinirizer(), random_state=42),\n","             ]\n","\n","    ensemble = Ensemble(classifiers=models, program=genetic_programming(), apply_model_replacement=apply_model_replacement)\n","    return ensemble\n","\n","def generate_ml_models(number_of_hidden_neurons, apply_model_replacement=False):\n","    models = [\n","              KNeighborsClassifier(5),\n","              KNeighborsClassifier(5),\n","              # DecisionTreeClassifier(),\n","              LogisticRegression(),\n","              LogisticRegression(),\n","              GaussianNB(),\n","              GaussianNB(),\n","              GaussianNB(),\n","              ]\n","    ensemble = Ensemble(classifiers=models, program=genetic_programming(), apply_model_replacement=apply_model_replacement)\n","    return ensemble"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EUqGh0r23YFM"},"outputs":[],"source":["# test set percentage\n","TESTSIZE=0.2\n","def get_reward(X,Y ,subset_features,apply_model_replacement=True):\n","    global TESTSIZE\n","    # index of selected features\n","    subset_features = np.where(np.array(subset_features) == 1)[0]\n","    if subset_features.shape[0] == 0:return 0\n","    # train test split\n","    X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=42, test_size=TESTSIZE)\n","\n","    # classifier = LogisticRegression()\n","\n","    ensemble1 = generate_oselm_models(number_of_hidden_neurons=X.shape[1]*3 // 2, apply_model_replacement=apply_model_replacement)\n","\n","    ensemble1.fit(X_train, y_train,np.where(subset_features != 1)[0])\n","\n","    y_pred1 = ensemble1.predict(X_test)\n","\n","    acc1 = accuracy_score(y_test, y_pred1)\n","\n","    return acc1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"unV82RQE3YFN"},"outputs":[],"source":["def reward_strategy(time_step: int, accuracy: float, accuracy_history: list, subset_features: list, error_rate: float,\n","                    beta: float = 0.99):\n","\n","    if sum(subset_features) == len(subset_features):\n","        return -5\n","\n","    elif sum(subset_features) == 0:\n","        return -10\n","\n","    elif accuracy > max(accuracy_history):\n","        return 0.5\n","\n","    elif accuracy < max(accuracy_history):\n","        return -0.1\n","\n","    else:\n","        return -1 * (beta * error_rate + ((1 - beta) * (sum(subset_features) / len(subset_features))))"]},{"cell_type":"markdown","metadata":{"id":"Crvb0m103YFN"},"source":["# 4. Some helper function\n","* 1- Object to store all information about the result like dataset name, accuracy, precision and etc...\n","* 2- Telegram api to send the result to a chat room\n","* 3- Send result to the chat room"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KvIWrubNBU4Z"},"outputs":[],"source":["class Results:\n","    def __init__(self, method_name, dataset_name, chunk_id, feature_space):\n","        self.dataset_name = dataset_name\n","        self.method_name = method_name\n","        self.chunk_id = chunk_id\n","        self.feature_space = feature_space\n","        self.feature_space_size = len(feature_space)\n","        self.result_information = {}\n","\n","    def set_chunk_id(self, chunk_id: int):\n","        self.chunk_id = chunk_id\n","\n","    def set_feature_space(self, feature_space: list):\n","        self.feature_space = feature_space\n","        self.feature_space_size = len(feature_space)\n","\n","    def add_result(self, model_type:str, result:dict):\n","        self.result_information[model_type] = result\n","\n","    def save(self, path='feature_selection_results'):\n","        file_name = self.method_name + '_' + self.dataset_name + '_' + '{}'.format(self.chunk_id) + '.pkl'\n","        with open(os.path.join(path, file_name), 'wb') as file_:\n","            pickle.dump(self, file_, pickle.HIGHEST_PROTOCOL)\n","        return"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JbjJKACLRokl"},"outputs":[],"source":["class Telegram:\n","\n","    def __init__(self, bot_token):\n","        self.end_point = 'https://api.telegram.org/bot'\n","        self.token = bot_token\n","        self.full_endpoint = self.end_point + self.token + '/'\n","\n","    def __repr__(self):\n","        return 'your token is {}'.format(self.full_endpoint)\n","\n","    def send_message(self, chat_id, message):\n","        send_text = self.full_endpoint + 'sendMessage'\n","        data = {'chat_id': chat_id, 'text': message}\n","        response = requests.get(send_text, data=data)\n","        return response\n","\n","    def send_photo(self, chat_id, photo):\n","        url = self.full_endpoint + 'sendPhoto'\n","        data = {'chat_id': chat_id}\n","        files = {'photo': open(photo, 'rb')}\n","        response = requests.post(url, data=data, files=files)\n","        return response\n","\n","    def send_document(self, chat_id, file):\n","        url = self.full_endpoint + 'sendDocument'\n","        data = {'chat_id': chat_id}\n","        files = {'document':open(file, 'rb')}\n","        response = requests.post(url, data = data, files = files)\n","        return response\n","    def get_updates(self):\n","        url = self.full_endpoint + 'getUpdates'\n","        response = requests.get(url)\n","        return response\n","\n","    def get_file_information(self, file_id):\n","        url = f'https://api.telegram.org/bot{self.token}/getFile'\n","        response = requests.post(url,data = {\"file_id\":file_id})\n","        if response.status_code != 200:\n","            return {\"ok\":\"False\"}\n","        json_response = response.json()\n","        if json_response['ok'] == False:\n","            return {\"ok\":\"False\"}\n","        file_path = json_response['result']['file_path']\n","        file_information = requests.get(f'https://api.telegram.org/file/bot{self.token}/{file_path}')\n","        return file_information.text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k9ORuCN-Plsv"},"outputs":[],"source":["def send_results(telegram_api, results):\n","    telegram.send_message(1021388563, 'dataset name : {}'.format(results.dataset_name))\n","    telegram.send_message(1021388563, 'chunk id : {}'.format(results.chunk_id))\n","    telegram.send_message(1021388563, 'selected features : {}'.format(results.feature_space))\n","    telegram.send_message(1021388563, 'results')\n","    for key in  results.result_information.keys():\n","        telegram.send_message(1021388563, 'model tpye : {}'.format(key))\n","        telegram.send_message(1021388563, '{}'.format(results.result_information[key]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7MfHt0ifMBYc"},"outputs":[],"source":["def save_object(obj, filename,path):\n","    \"\"\"\n","    _ INPUT (obj) THE OBJECT WE NEED SAVW IT (filename) THE NAME OF OBJECT\n","    \"\"\"\n","    filename = os.path.join(path,filename)\n","    with open(filename+\".pkl\", 'wb') as outp:\n","        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n","    outp.close()\n","def load_object(filename,path):\n","    \"\"\"\n","    _ INPUT THE NAME OF OBJECT WE NEED LOAD IT\n","    \"\"\"\n","    filename = os.path.join(path,filename)\n","    with open(filename+\".pkl\", 'rb') as outp:\n","        loaded_object = pickle.load(outp)\n","    outp.close()\n","    return loaded_object"]},{"cell_type":"markdown","metadata":{"id":"WhBpgSFaSn9X"},"source":["# 5. Feature Selection Main Algorithm\n","* For each type of agents we need a specific feature selection algorithm with little difference between them.\n","* Evaluation function using different machine learning model based on selected feature\n","* Result Visulization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PubHgDBA3YFO"},"outputs":[],"source":["def softmax_distrbution(agents):\n","    contrbutions = []\n","    for agent in agents:\n","        contrbutions.append(agent.contrbution)\n","    return softmax(contrbutions)\n","\n","def random_forest_distrbution(X_train,Y_train,num_of_agents, num_of_samples=1000):#10000\n","    X = []\n","    y = []\n","    for i in range(num_of_samples):\n","        features_space = np.random.choice([0, 1], size=(num_of_agents,)).tolist()\n","        accuracy = get_reward(X_train,Y_train, features_space)\n","        X.append(features_space)\n","        y.append(accuracy)\n","    X = np.array(X)\n","    y = np.array(y)\n","    rf = RandomForestRegressor(n_estimators=15)\n","    rf.fit(X, y)\n","    return rf.feature_importances_.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aLiW-JOFzR1N"},"outputs":[],"source":["def prepare_data(csv_filename, target_column_name='class'):\n","    # read csv file\n","    df = pd.read_csv(csv_filename)\n","    df = df.iloc[:80000, :]\n","    column_names = df.columns.tolist()\n","    if target_column_name not in column_names:\n","        target_column_name = column_names[-1]\n","    # get unique value in target column\n","    unique_vlaues = sorted(df[target_column_name].unique().tolist())\n","    df[target_column_name] = df[target_column_name].apply(lambda x: 0 if x == unique_vlaues[0] else 1)\n","    df[target_column_name] = df[target_column_name].astype('int')\n","    # rename the column of the dataframe\n","    num_of_columns = len(column_names)\n","    df.columns = list(range(num_of_columns))\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9A0KUfP6Sogv"},"outputs":[],"source":["def feature_selection(algo_type, agents, X, Y, eposide=100):\n","\n","    # column_names = list(range(dataset.shape[1]))\n","    # column_names[-1] = 'class'\n","    # dataset.columns = column_names\n","    epsilon = 0.01\n","    features_space = []\n","    NUMBER_OF_AGENTS = X.shape[1]\n","\n","    # get contrbutions\n","    contrbutions = []\n","    if algo_type == 'random_forest':\n","        contrbutions = random_forest_distrbution(X,Y,NUMBER_OF_AGENTS)\n","    elif algo_type in ['single_agent', 'average']:\n","        contrbutions = [1] * NUMBER_OF_AGENTS\n","\n","    for i in tqdm(range(eposide)):\n","        # define the initial space\n","        features_space = np.random.choice([0, 1], size=(NUMBER_OF_AGENTS,)).tolist()\n","        # rewards history\n","        rewards = [0]\n","\n","        # get action of each agent to create new feature space\n","        next_feature_space = []\n","        # contrbution of each agent\n","\n","        if algo_type == 'softmax':\n","            contrbutions = softmax_distrbution(agents)\n","\n","        for t in range(0, NUMBER_OF_AGENTS):\n","            action = agents[t].make_action(np.array(features_space.copy()), epsilon_greedy, epsilon)\n","            next_feature_space.append(action)\n","            if algo_type == 'single_agent':\n","                features_space[t] = action\n","\n","\n","        # calculate the total accuracy of new state (new feature space) and distrbute it using softmax\n","\n","        # 1- get the accuracy using machine learning model trained in the current subset feature\n","        reward_as_accuracy = get_reward(X,Y, next_feature_space)\n","\n","        # 2- using the reward strategy map the accuracy value (reward_as_accuracy) to new reward value (reward_at_time_t)\n","        reward_at_time_t = reward_strategy(t, reward_as_accuracy, rewards, next_feature_space, 1 - reward_as_accuracy)\n","        # add the accuray of machine learning model to rewards list to use it in the mapping reward strategy.\n","        rewards.append(reward_as_accuracy)\n","\n","        # total reward = reward after mapping\n","        total_reward = reward_at_time_t\n","\n","\n","\n","        # add state and actions to agent buffer reply  and the reward which equals to contrbution of the agent*total reward\n","        transition = []\n","        for t in range(0, NUMBER_OF_AGENTS):\n","            transition.clear()\n","            # add current state (current feature space )\n","            feature_space_copy = features_space.copy()\n","            transition.append(feature_space_copy)\n","            # add agent's action to the transition\n","            action = next_feature_space[t]\n","            transition.append(action)\n","            # add new state (new feature space) into transition\n","            transition.append(next_feature_space)\n","            # add distrbuted reward to the transition\n","            transition.append(total_reward * contrbutions[t])\n","            # add the transition to reply buffer\n","            agents[t].reply_buffer.append(transition)\n","            if len(agents[t].reply_buffer) > 32 and i % 32 == 0:\n","                agents[t].update_evaluation_network()\n","        if i % 64 == 0:\n","            for agent in agents:\n","                if agent.fitted:\n","                    agent.update_target_network()\n","        epsilon = 0.97 * epsilon\n","    return next_feature_space"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AS5FDN_E0bPJ"},"outputs":[],"source":["def generate_new_samples(buffer, y_values, n=500, y_col='label'):\n","    if not y_col in buffer.columns.tolist():\n","      y_col = buffer.columns.tolist()[-1]\n","    if y_values.sum() == 0:\n","       return buffer[buffer[y_col] == 1].sample(n, random_state=41)[:, :-1].values, np.array([1] * n)\n","    else:\n","      return buffer[buffer[y_col] == 0].sample(n,random_state=41)[:, :-1].values, np.array([0] * n)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eCSg_xgUJFvK"},"outputs":[],"source":["def feature_evolving(evolving_matrix):\n","    \"\"\"\n","    evolving_matrix : list of random list\n","    \"\"\"\n","    random_index = np.random.randint(0, len(evolving_matrix), 1)[0]\n","    return evolving_matrix[random_index]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fssDIs2hfmJT"},"outputs":[],"source":["def save_pickle(obj, file_name):\n","  with open(file_name, 'wb') as f:\n","    pickle.dump(obj, f)\n","def load_pickle(file_name):\n","  with open(file_name, 'rb') as f:\n","    d = pickle.load(f)\n","  return d"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3DaNb5bJ3YFP"},"outputs":[],"source":["# model_evaluation(algorithm_type, f_name, chunk, result, 'target', chunk_id)\n","def model_evaluation(chunk_X, chunk_Y, selected_features):\n","    # ('SVM', svm.SVC(kernel='rbf', max_iter=8000, C=0.2, probability=True)),\n","    # ('KNN', KNeighborsClassifier(5)),\n","    # ('DecisionTree', DecisionTreeClassifier(random_state=42)),\n","    # ('RandomForest', RandomForestClassifier()),\n","    # ('LogRegression', LogisticRegression(max_iter=500))\n","\n","\n","    # index of selected features\n","    subset_features = np.where(np.array(selected_features) == 1)[0]\n","    if subset_features.shape[0] == 0:return 0\n","    # train test split\n","    X, Y = chunk_X, chunk_Y\n","    X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=42, test_size=TESTSIZE)\n","\n","    # data normalization.\n","    # std = StandardScaler()\n","    # X_train = std.fit_transform(X_train)\n","    # X_test = std.transform(X_test)\n","\n","    model = generate_oselm_models(number_of_hidden_neurons=X_train.shape[1]*3 // 2, apply_model_replacement=True)\n","\n","    # classification and model evaluation\n","    # y_predictes, f1, recall ,precision ,accuracy = [],[],[],[],[]\n","    # for model in models:\n","    # model_name, model_obj = model[0], model[1]\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_test)\n","    y_predictes = y_pred\n","    f1 = f1_score(y_test, y_pred)\n","    recall = recall_score(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred)\n","    # roc_auc.append(roc_auc_score(y_test, y_pred))\n","    accuracy = accuracy_score(y_test, y_pred)\n","    print(f1,recall,precision,accuracy)\n","    return f1,recall,precision,accuracy,y_predictes # ,np.mean(roc_auc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N0DK-w4NOqX4"},"outputs":[],"source":["def dynamic_feature_selection(chunk_X, chunk_Y,save_path,algorithm_type=['softmax','average','single_agent','random_forest'],chunk_number=0):\n","  AgentsSoftmax.agent_count,AgentsAverage.agent_count,AgentsSingle.agent_count,AgentsRegression.agent_count = 0,0,0,0\n","  softmax_agents,average_agents,single_agent_agents,random_forest_agents,result,over_all=[],[],[],[],[],[]\n","  NUM_OF_FEATURES = chunk_X.shape[1]\n","  for i in range(NUM_OF_FEATURES):\n","    if 'random_forest' in algorithm_type:\n","      softmax_agents.append(AgentsSoftmax(create_model(NUM_OF_FEATURES),NUM_OF_FEATURES))\n","    if 'average' in algorithm_type:\n","      average_agents.append(AgentsAverage(create_model(NUM_OF_FEATURES),NUM_OF_FEATURES))\n","    if 'single_agent' in algorithm_type:\n","      single_agent_agents.append(AgentsSingle(create_model(NUM_OF_FEATURES),NUM_OF_FEATURES))\n","    if 'softmax' in algorithm_type:\n","      random_forest_agents.append(AgentsRegression(create_model(NUM_OF_FEATURES),NUM_OF_FEATURES))\n","\n","\n","  softmax_results,average_results,single_agent_results,random_forest_results,voting_results = results_dic(save_path,chunk_number)\n","\n","  if 'softmax' in algorithm_type:\n","    softmax_result = feature_selection('softmax', softmax_agents,chunk_X, chunk_Y)\n","    while len(softmax_result)==0:\n","      softmax_result = feature_selection('softmax', softmax_agents,chunk_X, chunk_Y)\n","    f1, recall ,precision ,accuracy ,y_predicte = model_evaluation(chunk_X=chunk_X,chunk_Y=chunk_Y,selected_features=softmax_result)\n","    over_all.append(f1)\n","    save_object(softmax_result, \"softmax_mask_\"+str(chunk_number),save_path)\n","    softmax_results[\"f1\"].append(f1)\n","    softmax_results[\"recall\"].append(recall)\n","    softmax_results[\"precision\"].append(precision)\n","    softmax_results[\"accuracy\"].append(accuracy)\n","    softmax_results[\"y_predicte\"].append(y_predicte)\n","    save_object(softmax_results, \"softmax_results\",save_path)\n","\n","\n","  if 'average' in algorithm_type:\n","    average_result = feature_selection('average', average_agents,chunk_X, chunk_Y)\n","    while len(average_result)==0:\n","      average_result = feature_selection('average', average_agents,chunk_X, chunk_Y)\n","    f1, recall ,precision ,accuracy ,y_predicte = model_evaluation(chunk_X=chunk_X,chunk_Y=chunk_Y,selected_features=average_result)\n","    over_all.append(f1)\n","    save_object(average_result, \"average_mask_\"+str(chunk_number),save_path)\n","    average_results[\"f1\"].append(f1)\n","    average_results[\"recall\"].append(recall)\n","    average_results[\"precision\"].append(precision)\n","    average_results[\"accuracy\"].append(accuracy)\n","    average_results[\"y_predicte\"].append(y_predicte)\n","    save_object(average_results, \"average_results\",save_path)\n","\n","  if 'single_agent' in algorithm_type:\n","    single_agent_result = feature_selection('single_agent', single_agent_agents,chunk_X, chunk_Y)\n","    while len(single_agent_result)==0:\n","      single_agent_result = feature_selection('single_agent', single_agent_agents,chunk_X, chunk_Y)\n","    f1, recall ,precision  ,accuracy ,y_predictes = model_evaluation(chunk_X=chunk_X,chunk_Y=chunk_Y,selected_features=single_agent_result)\n","    over_all.append(f1)\n","    save_object(single_agent_result, \"single_agent_mask_\"+str(chunk_number),save_path)\n","    single_agent_results[\"f1\"].append(f1)\n","    single_agent_results[\"recall\"].append(recall)\n","    single_agent_results[\"precision\"].append(precision)\n","    single_agent_results[\"accuracy\"].append(accuracy)\n","    single_agent_results[\"y_predicte\"].append(y_predicte)\n","    save_object(single_agent_results, \"single_agent_results\",save_path)\n","\n","  if 'random_forest' in algorithm_type:\n","    random_forest_result = feature_selection('random_forest', random_forest_agents,chunk_X, chunk_Y)\n","    while len(random_forest_result)==0:\n","      random_forest_result = feature_selection('random_forest', random_forest_agents,chunk_X, chunk_Y)\n","    f1, recall ,precision  ,accuracy ,y_predicte = model_evaluation(chunk_X=chunk_X,chunk_Y=chunk_Y,selected_features=random_forest_result)\n","    over_all.append(f1)\n","    save_object(random_forest_result, \"random_forest_mask_\"+str(chunk_number),save_path)\n","    random_forest_results[\"f1\"].append(f1)\n","    random_forest_results[\"recall\"].append(recall)\n","    random_forest_results[\"precision\"].append(precision)\n","    random_forest_results[\"accuracy\"].append(accuracy)\n","    random_forest_results[\"y_predicte\"].append(y_predicte)\n","    save_object(random_forest_results, \"random_forest_results\",save_path)\n","\n","  for softmax,average,single,random in zip(softmax_result,average_result,single_agent_result,random_forest_result):\n","    sum_votes = sum([softmax,average,single,random])\n","    if sum_votes > (len(algorithm_type) // 2):result.append(1)\n","    elif sum_votes == (len(algorithm_type) // 2):\n","      rand = np.random.uniform(low=0,high=1)\n","      if rand >0.5:result.append(1)\n","      else:result.append(0)\n","    else:result.append(0)\n","\n","\n","  f1, recall ,precision  ,accuracy ,y_predicte = model_evaluation(chunk_X, chunk_Y, result)\n","  over_all.append(f1)\n","  save_object(result, \"voting_mask_\"+str(chunk_number),save_path)\n","  voting_results[\"f1\"].append(f1)\n","  voting_results[\"recall\"].append(recall)\n","  voting_results[\"precision\"].append(precision)\n","  voting_results[\"accuracy\"].append(accuracy)\n","  voting_results[\"y_predicte\"].append(y_predicte)\n","  save_object(voting_results, \"voting_results\",save_path)\n","\n","\n","  re_all = [softmax_result,average_result,single_agent_result,random_forest_result,result]\n","  print(re_all[over_all.index(max(over_all))])\n","  return re_all[over_all.index(max(over_all))]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2O-Uy8opyJjv"},"outputs":[],"source":["def main(f_name, result_save_path=\"\",ChunkNumber=0,feature_selection_type=''):\n","  # load the dataset and then process it\n","  datasets = {}\n","\n","  d = prepare_data(f_name)\n","  d = d.sample(frac=1, random_state=42)\n","  buffer = d.sample(n=5000)\n","  d.reset_index(inplace=True)\n","  d.replace([np.inf], 0, inplace=True)\n","  datasets[f_name.split('/')[-1]] = d\n","  results = {}\n","\n","  for key in tqdm(datasets.keys()):\n","      drift_location = {}\n","      results[key] = {'model_result': []}\n","      # convert dataset from dataframe to numpy array.\n","      data = datasets[key].values\n","      # split the data into features array and target array.\n","      X, Y = data[:, 0:-1], data[:, -1].astype('int')\n","      if not os.path.exists(\"{}_evolving_matrix.pkl\".format(result_save_path)):\n","        a2 = np.random.randint(low=0, high=X.shape[1], size = X.shape[1] // 6).tolist()\n","        a3 = np.random.randint(low=0, high=X.shape[1], size = X.shape[1] // 5).tolist()\n","        a4 = np.random.randint(low=0, high=X.shape[1], size = X.shape[1] // 4).tolist()\n","        evolving_matrix = [a2, a3, a4]\n","        save_pickle(evolving_matrix, \"{}_evolving_matrix.pkl\".format(result_save_path))\n","      else:\n","        evolving_matrix = load_pickle(\"{}_evolving_matrix.pkl\".format(result_save_path))\n","      chunks_features = np.array_split(X, 10)\n","      chunks_labels = np.array_split(Y, 10)\n","\n","\n","      # result_save_path_data = os.path.join(result_save_path, key)\n","\n","      ################# train on each chunk ####################\n","      print(\"===================== dataset : {} ======================\".format(key))\n","      for CN,chunk_X, chunk_Y in tqdm(zip([*range(len(chunks_labels))],chunks_features, chunks_labels)):\n","          if ChunkNumber > CN:\n","            print(\"We Skip Chunk Number : {}\".format(CN))\n","            continue\n","          try:\n","            chunk_X, chunk_Y = SMOTE().fit_resample(chunk_X, chunk_Y)\n","          except:\n","            if chunk_Y.sum() in [0, 1]:\n","              new_samples, new_labels = generate_new_samples(buffer, chunk_Y)\n","              chunk_X = np.concatenate((chunk_X, new_samples))\n","              chunk_Y = np.concatenate((chunk_Y, new_labels))\n","          gc.collect()\n","          if feature_selection_type == \"feature_evolving\":\n","            unselected_feautres = feature_evolving(evolving_matrix=evolving_matrix)\n","            chunk_X = np.delete(chunk_X, unselected_feautres, 1)\n","          selected = dynamic_feature_selection(chunk_X=chunk_X, chunk_Y=chunk_Y,save_path=result_save_path,chunk_number=CN)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":54,"status":"ok","timestamp":1690696141227,"user":{"displayName":"Fatrmah rac","userId":"03011577188967940535"},"user_tz":-180},"id":"DnzKMN2W3YFP","outputId":"ccf6be0a-7372-4b6b-8c15-33eca072ce5f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/drive/My Drive/Colab Notebooks/Muawiya/Genetic Programming Combiner with DFS/data/kddcup99_csv.csv',\n"," '/content/drive/My Drive/Colab Notebooks/Muawiya/Genetic Programming Combiner with DFS/data/ISCX2012.csv',\n"," '/content/drive/My Drive/Colab Notebooks/Muawiya/Genetic Programming Combiner with DFS/data/CSE-CIC2018.csv',\n"," '/content/drive/My Drive/Colab Notebooks/Muawiya/Genetic Programming Combiner with DFS/data/CICIDS2017.csv',\n"," '/content/drive/My Drive/Colab Notebooks/Muawiya/Genetic Programming Combiner with DFS/data/7recurrentDrift.csv',\n"," '/content/drive/My Drive/Colab Notebooks/Muawiya/Genetic Programming Combiner with DFS/data/blip.csv',\n"," '/content/drive/My Drive/Colab Notebooks/Muawiya/Genetic Programming Combiner with DFS/data/incrementalDrift.csv',\n"," '/content/drive/My Drive/Colab Notebooks/Muawiya/Genetic Programming Combiner with DFS/data/7gradualDrift.csv',\n"," '/content/drive/My Drive/Colab Notebooks/Muawiya/Genetic Programming Combiner with DFS/data/7suddenDrift.csv']"]},"metadata":{},"execution_count":30}],"source":["filenames = ['kddcup99_csv.csv','ISCX2012.csv','CSE-CIC2018.csv','CICIDS2017.csv','7recurrentDrift.csv', 'blip.csv', 'incrementalDrift.csv',\n","             '7gradualDrift.csv', '7suddenDrift.csv']\n","filenames = list(map(lambda x: os.path.join(data_path, x), filenames))\n","filenames"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iC4Go5kD3YFQ"},"outputs":[],"source":["def results_dic(path,chunk_number=0):\n","  if chunk_number == 0 :\n","    softmax_results = {\"f1\":[],\"recall\":[],\"precision\":[],\"accuracy\":[],\"y_predicte\":[]}\n","    average_results = {\"f1\":[],\"recall\":[],\"precision\":[],\"accuracy\":[],\"y_predicte\":[]}\n","    single_agent_results = {\"f1\":[],\"recall\":[],\"precision\":[],\"accuracy\":[],\"y_predicte\":[]}\n","    random_forest_results = {\"f1\":[],\"recall\":[],\"precision\":[],\"accuracy\":[],\"y_predicte\":[]}\n","    voting_results = {\"f1\":[],\"recall\":[],\"precision\":[],\"accuracy\":[],\"y_predicte\":[]}\n","  else:\n","    softmax_results = load_object('softmax_results',path)\n","    average_results = load_object('average_results',path)\n","    single_agent_results = load_object('single_agent_results',path)\n","    random_forest_results = load_object('random_forest_results',path)\n","    voting_results = load_object('voting_results',path)\n","  return softmax_results,average_results,single_agent_results,random_forest_results,voting_results"]},{"cell_type":"code","source":["# # data_name = ['kddcup99','ISCX2012','CSE-CIC2018','CICIDS2017','7recurrentDrift', 'blip', 'incrementalDrift','7gradualDrift', '7suddenDrift']\n","# # for f_name,d_name in zip(filenames,data_name):\n","# #   path = os.path.join(feature_selection_results,d_name)\n","# #   os.makedirs(path, exist_ok=True)\n","# #   main(f_name, result_save_path=path)"],"metadata":{"id":"D-IW7zzcu03X"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HNGUVbpfPMid"},"outputs":[],"source":["# i=0\n","# data_name = ['kddcup99','ISCX2012','CSE-CIC2018','CICIDS2017','7recurrentDrift', 'blip', 'incrementalDrift','7gradualDrift', '7suddenDrift']\n","# f_name,d_name = filenames[i],data_name[i]\n","# path = os.path.join(feature_selection_results,d_name)\n","# os.makedirs(path, exist_ok=True)\n","# main(f_name, result_save_path=path,ChunkNumber=7)\n","# gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C4JJBA9iyp6T"},"outputs":[],"source":["# i=1\n","# data_name = ['kddcup99','ISCX2012','CSE-CIC2018','CICIDS2017','7recurrentDrift', 'blip', 'incrementalDrift','7gradualDrift', '7suddenDrift']\n","# f_name,d_name = filenames[i],data_name[i]\n","# path = os.path.join(feature_selection_results,d_name)\n","# os.makedirs(path, exist_ok=True)\n","# main(f_name, result_save_path=path,ChunkNumber=9)\n","# gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-qSF-tfjz67N"},"outputs":[],"source":["# i=2\n","# data_name = ['kddcup99','ISCX2012','CSE-CIC2018','CICIDS2017','7recurrentDrift', 'blip', 'incrementalDrift','7gradualDrift', '7suddenDrift']\n","# f_name,d_name = filenames[i],data_name[i]\n","# path = os.path.join(feature_selection_results,d_name)\n","# os.makedirs(path, exist_ok=True)\n","# main(f_name, result_save_path=path,ChunkNumber=7)\n","# gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XCevy61J3oSK"},"outputs":[],"source":["i=3\n","data_name = ['kddcup99','ISCX2012','CSE-CIC2018','CICIDS2017','7recurrentDrift', 'blip', 'incrementalDrift','7gradualDrift', '7suddenDrift']\n","f_name,d_name = filenames[i],data_name[i]\n","path = os.path.join(feature_selection_results,d_name)\n","os.makedirs(path, exist_ok=True)\n","main(f_name, result_save_path=path,ChunkNumber=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ji23x-vU3oU0"},"outputs":[],"source":["# ChunkNumber = 0\n","i=4\n","data_name = ['kddcup99','ISCX2012','CSE-CIC2018','CICIDS2017','7recurrentDrift', 'blip', 'incrementalDrift','7gradualDrift', '7suddenDrift']\n","f_name,d_name = filenames[i],data_name[i]\n","path = os.path.join(feature_selection_results,d_name)\n","os.makedirs(path, exist_ok=True)\n","main(f_name, result_save_path=path,ChunkNumber=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kjTZ6zy_3oXr"},"outputs":[],"source":["# ChunkNumber = 0\n","i=5\n","data_name = ['kddcup99','ISCX2012','CSE-CIC2018','CICIDS2017','7recurrentDrift', 'blip', 'incrementalDrift','7gradualDrift', '7suddenDrift']\n","f_name,d_name = filenames[i],data_name[i]\n","path = os.path.join(feature_selection_results,d_name)\n","os.makedirs(path, exist_ok=True)\n","main(f_name, result_save_path=path,ChunkNumber=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xEGAvHN33quN"},"outputs":[],"source":["# ChunkNumber = 0\n","i=6\n","data_name = ['kddcup99','ISCX2012','CSE-CIC2018','CICIDS2017','7recurrentDrift', 'blip', 'incrementalDrift','7gradualDrift', '7suddenDrift']\n","f_name,d_name = filenames[i],data_name[i]\n","path = os.path.join(feature_selection_results,d_name)\n","os.makedirs(path, exist_ok=True)\n","main(f_name, result_save_path=path,ChunkNumber=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YaxLOIFy3qwU"},"outputs":[],"source":["# ChunkNumber = 0\n","i=7\n","data_name = ['kddcup99','ISCX2012','CSE-CIC2018','CICIDS2017','7recurrentDrift', 'blip', 'incrementalDrift','7gradualDrift', '7suddenDrift']\n","f_name,d_name = filenames[i],data_name[i]\n","path = os.path.join(feature_selection_results,d_name)\n","os.makedirs(path, exist_ok=True)\n","main(f_name, result_save_path=path,ChunkNumber=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F0h9lHMB3qyt"},"outputs":[],"source":["# ChunkNumber = 0\n","i=8\n","data_name = ['kddcup99','ISCX2012','CSE-CIC2018','CICIDS2017','7recurrentDrift', 'blip', 'incrementalDrift','7gradualDrift', '7suddenDrift']\n","f_name,d_name = filenames[i],data_name[i]\n","path = os.path.join(feature_selection_results,d_name)\n","os.makedirs(path, exist_ok=True)\n","main(f_name, result_save_path=path,ChunkNumber=0)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}