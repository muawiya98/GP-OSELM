{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"hrQXfAEAWiXu"},"outputs":[],"source":["!pip install -q scikit-multiflow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5AYERK4_cyyj"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from skmultiflow.drift_detection import ADWIN, DDM\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.preprocessing import OneHotEncoder\n","from matplotlib.colors import ListedColormap\n","from multiprocessing.pool import ThreadPool\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import roc_auc_score\n","from imblearn.over_sampling import SMOTE\n","from sklearn.utils import shuffle\n","from contextlib import suppress\n","from collections import Counter\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","from random import shuffle\n","import seaborn as sns\n","from time import time\n","import pandas as pd\n","import numpy as np\n","import warnings\n","import scipy.io\n","import pickle\n","import sys\n","import gc\n","import os\n","import re"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TBvNwKEy3OxE"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6cX3kM3dKNyC"},"outputs":[],"source":["data_path = '/content/drive/My Drive/Colab Notebooks/Muawiya/Genetic Programming Combiner with DFS/data'\n","code_path = '/content/drive/My Drive/Colab Notebooks/Muawiya/Genetic Programming Combiner with DFS/Codes/Shared Codes'\n","results_path = '/content/drive/My Drive/Colab Notebooks/Muawiya/Genetic Programming Combiner with DFS/results'\n","feature_selection_results = '/content/drive/My Drive/Colab Notebooks/Muawiya/Genetic Programming Combiner with DFS/feature_selection_results'\n","feature_selection_results_evolving = '/content/drive/My Drive/Colab Notebooks/Muawiya/Genetic Programming Combiner with DFS/feature_selection_results_evolving'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"haULZUPaXxm_"},"outputs":[],"source":["sys.path.insert(0,code_path)\n","from genetic_programming import SymbolicRegressor\n","from binirizer import CustomLabelBinirizer\n","from ensemble import Ensemble, Classifier\n","from oselm import OSELMClassifier,set_use_know\n","from DynamicFeatureSelection import dynamic_feature_selection\n","from SharedFunctions import prepare_data,train_and_test,feature_evolving,save_pickle,load_pickle,save_object,load_object,generate_new_samples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qdn3Zzj9eS3m"},"outputs":[],"source":["filenames = ['kddcup99_csv.csv','ISCX2012.csv','CSE-CIC2018.csv','CICIDS2017.csv','7recurrentDrift.csv', 'blip.csv', 'incrementalDrift.csv',\n","             '7gradualDrift.csv', '7suddenDrift.csv']\n","filenames = list(map(lambda x: os.path.join(data_path, x), filenames))\n","filenames"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"voLanNr9zsZE"},"outputs":[],"source":["def genetic_programming():\n","    return SymbolicRegressor(population_size=10,\n","            generations=5, stopping_criteria=0.85,\n","            p_crossover=0.7, p_subtree_mutation=0.1,\n","            p_hoist_mutation=0.05, p_point_mutation=0.1,\n","            max_samples=0.7, verbose=1,\n","            parsimony_coefficient=1e-4, random_state=42,\n","            function_set=['avg2', 'avg3', 'avg5',\n","                          'median3', 'median5', 'maximum2', 'maximum3', 'maximum5'],\n","            metric='f1-score')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yMFnEvVclk0p"},"outputs":[],"source":["def generate_oselm_models(number_of_hidden_neurons, apply_model_replacement=False):\n","    models= [OSELMClassifier(number_of_hidden_neurons, 'relu', binarizer=CustomLabelBinirizer(), random_state=42),\n","             OSELMClassifier(number_of_hidden_neurons, 'relu', binarizer=CustomLabelBinirizer(), random_state=42),\n","             OSELMClassifier(number_of_hidden_neurons, 'relu', binarizer=CustomLabelBinirizer(), random_state=42),\n","             OSELMClassifier(number_of_hidden_neurons, 'relu', binarizer=CustomLabelBinirizer(), random_state=42),\n","             ]\n","\n","    ensemble = Ensemble(classifiers=models, program=genetic_programming(), apply_model_replacement=apply_model_replacement)\n","    return ensemble\n","\n","def generate_ml_models(number_of_hidden_neurons, apply_model_replacement=False):\n","    models = [\n","              KNeighborsClassifier(5),\n","              KNeighborsClassifier(5),\n","              # DecisionTreeClassifier(),\n","              LogisticRegression(),\n","              LogisticRegression(),\n","              GaussianNB(),\n","              GaussianNB(),\n","              GaussianNB(),\n","              ]\n","    ensemble = Ensemble(classifiers=models, program=genetic_programming(), apply_model_replacement=apply_model_replacement)\n","    return ensemble"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EPZg9p6YzeRa"},"outputs":[],"source":["def concept_drift_detection(drift_detection_obj, sample) -> bool:\n","    \"\"\"\n","    Detect concept drift\n","    :param drift_detection_obj: sklearn drift detection object (ADWIN, DDM, )\n","    :param smaple : new instanece of data stream\n","    return True if concept drift was detected otherwise false\n","    \"\"\"\n","    drift_detection_obj.add_element(sample)\n","    return drift_detection_obj.detected_change()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4W42BGI8nLlN"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.feature_selection import SelectFromModel\n","def random_forest_feature_selection(X, y):\n","    \"\"\"\n","    return best feature from X using random forest\n","    \"\"\"\n","    sel = SelectFromModel(RandomForestClassifier(n_estimators = 20))\n","    sel.fit(X, y)\n","    return sel.get_support()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nx-BHuxdo1J2"},"outputs":[],"source":["def E2SC4ID (X,\n","             y,\n","             sample_index:int,\n","             buffer:list,\n","             ensemble: Ensemble,\n","             drift_detection_obj,\n","             maxC=8,\n","             n=2000,\n","             train_size=0.5,\n","             drift=False,\n","             unselected_features=None,\n","             drift_location={}):\n","    y_pred = ensemble.global_support_degree(X)\n","    # if the sample is labeled then insert it into buffer\n","    if y is not None:\n","        buffer.append((X, y))\n","        actual_drift = concept_drift_detection(drift_detection_obj, int(y!=y_pred))\n","        if actual_drift and not drift:\n","          drift_location[sample_index] = 'drift'\n","        drift = drift or actual_drift\n","        # if drift:\n","        #   print(\"sample_index {} => drift {}\".format(sample_index,drift))\n","        if len(buffer) >= n:\n","            print(\"buffer size {} N {}\".format(len(buffer),n))\n","            print(\"drift {}\".format(drift))\n","            if drift:\n","                drift = False\n","                drift_detection_obj.reset()\n","                x_buffer, y_buffer = [], []\n","                for tup in buffer:\n","                    x_buffer.append(tup[0])\n","                    y_buffer.append(tup[1])\n","                ######################################################\n","                train_size = int(len(x_buffer)*train_size)\n","                X_train = x_buffer[:train_size]\n","                y_train = y_buffer[:train_size]\n","                X_valid = x_buffer[train_size:]\n","                y_valid = y_buffer[train_size:]\n","                ######################################################\n","                __sum = np.array(y_train).sum()\n","                print(\"len of y_trian {} number of ones {}\".format(len(y_train),__sum))\n","                if 0 ==  __sum or __sum == len(y_train):\n","                  y_train[0] = 0 if y_train[0] == 1 else 1\n","                new_models = ensemble.classifier_induction([\n","                                        model.clf for model in ensemble.classifiers],\n","                                        X_train,\n","                                        y_train,\n","                                        unselected_features)\n","                if len(ensemble.classifiers) > maxC:\n","                    ensemble.model_replacement('time')\n","                ######################################################\n","                ensemble.update_program(X_valid, y_valid)\n","            else:\n","                buffer.clear()\n","                print(\"Clear bufer : {}\".format(len(buffer)))\n","        return ensemble, buffer, drift, drift_location\n","    else:\n","      print(\"y is none \")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bzPNev5QnM6s"},"outputs":[],"source":["def E2SC4ID_STREAM(ensemble, stremdata, y, unselected_features, drift_location, chunk_number,result_save_path_data,key,sample_number=0):\n","    if not ensemble.fitted:\n","      y[0] = 0 if y[0] == 1 else 1\n","      ensemble.fit(stremdata[:200], y[:200])\n","\n","    drift_detection_obj, drift, buffer = DDM(),False,[]\n","    if sample_number != 0 :\n","      drift_location = load_pickle(os.path.join(result_save_path_data, \"{}_drift_location.pkl\".format(key)))\n","      drift = load_pickle(os.path.join(result_save_path_data, \"{}_drift.pkl\".format(key)))\n","      buffer = load_pickle(os.path.join(result_save_path_data, \"{}_buffer.pkl\".format(key)))\n","      ensemble = load_pickle(os.path.join(result_save_path_data, \"{}_ensemble.pkl\".format(key)))\n","\n","    for i in tqdm(range(200+sample_number, len(stremdata))):\n","        X, y_true = stremdata[i], y[i]\n","        ensemble, buffer, drift, drift_location = E2SC4ID (X,\n","                                                           y_true,\n","                                                           sample_index=(i +(chunk_number * 10000)),\n","                                                           buffer=buffer,\n","                                                           ensemble=ensemble,\n","                                                           drift_detection_obj=drift_detection_obj,\n","                                                           maxC=8,\n","                                                           n=len(stremdata)-200,\n","                                                           train_size=0.7,\n","                                                           drift=drift,\n","                                                           unselected_features=unselected_features,\n","                                                           drift_location=drift_location)\n","\n","        save_pickle(drift, os.path.join(result_save_path_data, \"{}_drift.pkl\".format(key)))\n","        save_pickle(buffer, os.path.join(result_save_path_data, \"{}_buffer.pkl\".format(key)))\n","        save_pickle(drift_location, os.path.join(result_save_path_data, \"{}_drift_location.pkl\".format(key)))\n","        save_pickle(ensemble, os.path.join(result_save_path_data, \"{}_ensemble.pkl\".format(key)))\n","    return ensemble, drift_location"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DHuWedeLnM3W"},"outputs":[],"source":["def main(f_name, generate_model, train_size=3000,apply_model_replacement=False, transfer_learning=False, feature_selection=\"random_forest\", result_save_path=\"\",ChunkNumber=0,sample_number=0,DFS_results_path=''):\n","  \"\"\"\n","  datasets_paths: list of paths (absolute path for each dataset)\n","  generate_model: function to generate machine learning model.\n","  train_size: number of samples to be used in training phase.\n","  transfer_learning: to determine using of transfer learning in the training phase.\n","  \"\"\"\n","  # load the dataset and then process it\n","  datasets = {}\n","  first_chunk=True\n","  d = prepare_data(f_name)\n","  d = d.sample(frac=1, random_state=42)\n","  buffer = d.sample(n=5000)\n","  d.reset_index(inplace=True)\n","  d.replace([np.inf], 0, inplace=True)\n","  datasets[f_name.split('/')[-1]] = d\n","  results = {}\n","  drift_locations_in_all_dataset = {}\n","  ensemble = None ####\n","  ########################################\n","  for key in tqdm(datasets.keys()):\n","      result_save_path_data = os.path.join(result_save_path, key)\n","      drift_location = {} if ChunkNumber==0 else load_pickle(os.path.join(result_save_path_data, \"{}_drift_location.pkl\".format(key)))\n","      results[key] = {'model_result': []}\n","      # convert dataset from dataframe to numpy array.\n","      data = datasets[key].values\n","      # split the data into features array and target array.\n","      X, Y = data[:, 0:-1], data[:, -1].astype('int')\n","      if not os.path.exists(\"{}_evolving_matrix.pkl\".format(key)):\n","        a2 = np.random.randint(low=0, high=X.shape[1], size = X.shape[1] // 6).tolist()\n","        a3 = np.random.randint(low=0, high=X.shape[1], size = X.shape[1] // 5).tolist()\n","        a4 = np.random.randint(low=0, high=X.shape[1], size = X.shape[1] // 4).tolist()\n","        evolving_matrix = [a2, a3, a4]\n","        save_pickle(evolving_matrix, \"{}_evolving_matrix.pkl\".format(key))\n","      else:\n","        evolving_matrix = load_pickle(\"{}_evolving_matrix.pkl\".format(key))\n","      ensemble = generate_model(number_of_hidden_neurons=X.shape[1]*3 // 2, apply_model_replacement=apply_model_replacement)\n","      # split the data into chunks (10 chunks)\n","      chunks_features = np.array_split(X, 10)\n","      chunks_labels = np.array_split(Y, 10)\n","\n","      ################# train on each chunk ####################\n","      print(\"===================== dataset : {} ======================\".format(key))\n","      chunk_number = 1\n","      if ChunkNumber>0:\n","        ensemble = load_pickle(os.path.join(result_save_path_data, \"{}_ensemble.pkl\".format(key)))\n","        results = load_pickle(os.path.join(result_save_path_data, \"{}_results.pkl\".format(key)))\n","        drift_locations_in_all_dataset = load_pickle(os.path.join(result_save_path_data, \"{}_drift_locations_in_all_dataset.pkl\".format(key)))\n","      for CN,chunk_X, chunk_Y in tqdm(zip([*range(len(chunks_labels))],chunks_features, chunks_labels)):\n","          if ChunkNumber > CN:\n","            print(\"Skip Chunk Number : {}\".format(CN))\n","            continue\n","          if not first_chunk:\n","            sample_number=0\n","            first_chunk = False\n","          else:first_chunk = False\n","          try:\n","            chunk_X, chunk_Y = SMOTE().fit_resample(chunk_X, chunk_Y)\n","          except:\n","            if chunk_Y.sum() in [0, 1]:\n","              new_samples, new_labels = generate_new_samples(buffer, chunk_Y)\n","              chunk_X = np.concatenate((chunk_X, new_samples))\n","              chunk_Y = np.concatenate((chunk_Y, new_labels))\n","          gc.collect()\n","          unselected_feautres = None\n","          selected = None\n","          X_train, X_test, y_train, y_test = chunk_X[:train_size], chunk_X[train_size:], chunk_Y[:train_size], chunk_Y[train_size:]\n","          print(\"X_train shape {} , X_test shape {} \".format(X_train.shape,X_test.shape))\n","          if feature_selection[0] == \"feature_evolving\":\n","            unselected_feautres = feature_evolving(evolving_matrix=evolving_matrix)\n","            if feature_selection[1] == \"random_forest\":\n","              print('random_forest')\n","              print(\"1- X_trian[{}] {} shape {} \".format(0,X_train[0],X_train[0].shape))\n","              X_train = np.delete(X_train, unselected_feautres, 1)\n","              print(\"2- X_trian[{}] {} shape {} \".format(0,X_train[0],X_train[0].shape))\n","              X_test = np.delete(X_test, unselected_feautres, 1)\n","              selected = random_forest_feature_selection(X_train, y_train)\n","              unselected_feautres = np.where(selected != 1)[0]\n","            elif feature_selection[1] == \"DFS_feature_selection\":\n","              # selected = dynamic_feature_selection(chunk_X, chunk_Y)\n","              unselected_feautres = feature_evolving(evolving_matrix=load_pickle(\"{}_evolving_matrix.pkl\".format(DFS_results_path)))\n","              X_train = np.delete(X_train, unselected_feautres, 1)\n","              X_test = np.delete(X_test, unselected_feautres, 1)\n","              softmax_results = load_object('softmax_results',DFS_results_path)\n","              average_results = load_object('average_results',DFS_results_path)\n","              single_agent_results = load_object('single_agent_results',DFS_results_path)\n","              random_forest_results = load_object('random_forest_results',DFS_results_path)\n","              voting_results = load_object('voting_results',DFS_results_path)\n","              f1_score_for_all_algorithm = [softmax_results['f1'],average_results['f1'],single_agent_results['f1'],random_forest_results['f1'],voting_results['f1']]\n","              masks = [load_object(\"softmax_mask_\"+str(CN),DFS_results_path),\n","                        load_object(\"average_mask_\"+str(CN),DFS_results_path),\n","                        load_object(\"single_agent_mask_\"+str(CN),DFS_results_path),\n","                        load_object(\"random_forest_mask_\"+str(CN),DFS_results_path),\n","                        load_object(\"voting_mask_\"+str(CN),DFS_results_path)]\n","              selected = masks[f1_score_for_all_algorithm.index(max(f1_score_for_all_algorithm))]\n","              unselected_feautres = np.where(selected != 1)[0]\n","          else:\n","            if feature_selection[1] == \"random_forest\":\n","              selected = random_forest_feature_selection(X_train, y_train)\n","              unselected_feautres = np.where(selected != 1)[0]\n","            elif feature_selection[1] == \"DFS_feature_selection\":\n","              # selected = dynamic_feature_selection(X_train, y_train)\n","              softmax_results = load_object('softmax_results',DFS_results_path)\n","              average_results = load_object('average_results',DFS_results_path)\n","              single_agent_results = load_object('single_agent_results',DFS_results_path)\n","              random_forest_results = load_object('random_forest_results',DFS_results_path)\n","              voting_results = load_object('voting_results',DFS_results_path)\n","              f1_score_for_all_algorithm = [softmax_results['f1'][CN],\n","                                            average_results['f1'][CN],\n","                                            single_agent_results['f1'][CN],\n","                                            random_forest_results['f1'][CN],\n","                                            voting_results['f1'][CN]]\n","              masks = [load_object(\"softmax_mask_\"+str(CN),DFS_results_path),\n","                        load_object(\"average_mask_\"+str(CN),DFS_results_path),\n","                        load_object(\"single_agent_mask_\"+str(CN),DFS_results_path),\n","                        load_object(\"random_forest_mask_\"+str(CN),DFS_results_path),\n","                        load_object(\"voting_mask_\"+str(CN),DFS_results_path)]\n","              selected = masks[f1_score_for_all_algorithm.index(max(f1_score_for_all_algorithm))]\n","              unselected_feautres = np.where(selected != 1)[0]\n","          print(\"selected : \",sum(selected))\n","          if not os.path.exists(result_save_path_data):\n","            os.mkdir(result_save_path_data)\n","          if transfer_learning:\n","            temp = np.squeeze(X_train[:, selected]) if len(list(X_train[:, selected].shape))>2 else X_train[:, selected]\n","            print(selected)\n","            print(\"3- X_trian[{}] {} shape {}\".format(0,temp[0],temp[0].shape))\n","            # temp = np.squeeze(X_train) if len(list(X_train.shape))>2 else X_train\n","            ensemble, drift_location = E2SC4ID_STREAM(ensemble=ensemble, stremdata=temp, y=y_train, unselected_features=None, drift_location=drift_location,\n","                                                      chunk_number=chunk_number, result_save_path_data=result_save_path_data,key=key,sample_number=sample_number)\n","            temp = np.squeeze(X_test[:, selected]) if len(list(X_test[:, selected].shape))>2 else X_test[:, selected]\n","            # temp = np.squeeze(X_test) if len(list(X_test.shape))>2 else X_test\n","\n","            ensemble.evaluate(temp, y_test, chunk_number)\n","          else:\n","            ensemble, drift_location = E2SC4ID_STREAM(ensemble=generate_model(number_of_hidden_neurons=X.shape[1]*3 // 2,apply_model_replacement=apply_model_replacement),\n","                                                      stremdata=X_train, y=y_train, unselected_features=unselected_feautres, drift_location=drift_location,\n","                                                      chunk_number=chunk_number,result_save_path_data=result_save_path_data,key=key,sample_number=sample_number)\n","            ensemble.evaluate(X_test, y_test, chunk_number)\n","          save_pickle(drift_location, os.path.join(result_save_path_data, \"{}_drift_location.pkl\".format(key)))\n","\n","          temp = np.squeeze(X_test[:, selected]) if len(list(X_test[:, selected].shape))>2 else X_test[:, selected]\n","          y_pre = ensemble.predict(temp)\n","          results[key][chunk_number] = {\"y_true\" : y_test, \"y_pred\": y_pre}\n","          results[key]['model_result'].append(ensemble.scores)\n","          if transfer_learning:\n","             ensemble.fit(temp, y_test, None)\n","          chunk_number += 1\n","          drift_locations_in_all_dataset[key] = drift_location\n","\n","          save_pickle(ensemble, os.path.join(result_save_path_data, \"{}_ensemble.pkl\".format(key)))\n","          save_pickle(results, os.path.join(result_save_path_data, \"{}_results.pkl\".format(key)))\n","          save_pickle(drift_locations_in_all_dataset, os.path.join(result_save_path_data, \"{}_drift_locations_in_all_dataset.pkl\".format(key)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h_tIj-P-_eCF"},"outputs":[],"source":["data_name = ['kddcup99','ISCX2012','CSE-CIC2018','CICIDS2017','7recurrentDrift', 'blip', 'incrementalDrift','7gradualDrift', '7suddenDrift']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gc8hLC1ssBY4"},"outputs":[],"source":["# # use_know = True\n","# # set_use_know(use_know)\n","# # path = os.path.join(results_path,'kpfaoselm_gp_combiner_with_model_replacement_dynamic_feature_selection')\n","# # os.makedirs(path, exist_ok=True)\n","# # for f_name,d_name in zip(filenames,data_name):\n","# #   DFS_results_path = os.path.join(feature_selection_results,d_name)\n","# #   main(f_name, generate_oselm_models, apply_model_replacement=True,transfer_learning=True,\n","# #       feature_selection=[\"with_out_feature_evolving\",'DFS_feature_selection'], result_save_path=path,\n","# #       DFS_results_path=DFS_results_path,ChunkNumber=ChunkNumber,sample_number=sample_number)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"af2hmL7Pupg9"},"outputs":[],"source":["DATA_NUMBER=0\n","ChunkNumber=0\n","sample_number=sum([0])\n","use_know = True\n","set_use_know(use_know)\n","path = os.path.join(results_path,'kpfaoselm_gp_combiner_with_model_replacement_dynamic_feature_selection')\n","os.makedirs(path, exist_ok=True)\n","f_name,d_name = filenames[DATA_NUMBER],data_name[DATA_NUMBER]\n","DFS_results_path = os.path.join(feature_selection_results,d_name)\n","main(f_name, generate_oselm_models, apply_model_replacement=True,transfer_learning=True,\n","     feature_selection=[\"with_out_feature_evolving\",'DFS_feature_selection'], result_save_path=path,\n","     DFS_results_path=DFS_results_path,ChunkNumber=ChunkNumber,sample_number=sample_number)"]},{"cell_type":"code","source":["DATA_NUMBER=1\n","ChunkNumber=0\n","sample_number=sum([0])\n","use_know = True\n","set_use_know(use_know)\n","path = os.path.join(results_path,'kpfaoselm_gp_combiner_with_model_replacement_dynamic_feature_selection')\n","os.makedirs(path, exist_ok=True)\n","f_name,d_name = filenames[DATA_NUMBER],data_name[DATA_NUMBER]\n","DFS_results_path = os.path.join(feature_selection_results,d_name)\n","main(f_name, generate_oselm_models, apply_model_replacement=True,transfer_learning=True,\n","     feature_selection=[\"with_out_feature_evolving\",'DFS_feature_selection'], result_save_path=path,\n","     DFS_results_path=DFS_results_path,ChunkNumber=ChunkNumber,sample_number=sample_number)"],"metadata":{"id":"zU9mZ3OP1NZm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DATA_NUMBER=2\n","ChunkNumber=0\n","sample_number=sum([0])\n","use_know = True\n","set_use_know(use_know)\n","path = os.path.join(results_path,'kpfaoselm_gp_combiner_with_model_replacement_dynamic_feature_selection')\n","os.makedirs(path, exist_ok=True)\n","f_name,d_name = filenames[DATA_NUMBER],data_name[DATA_NUMBER]\n","DFS_results_path = os.path.join(feature_selection_results,d_name)\n","main(f_name, generate_oselm_models, apply_model_replacement=True,transfer_learning=True,\n","     feature_selection=[\"with_out_feature_evolving\",'DFS_feature_selection'], result_save_path=path,\n","     DFS_results_path=DFS_results_path,ChunkNumber=ChunkNumber,sample_number=sample_number)"],"metadata":{"id":"ucjx1_yt1NcA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DATA_NUMBER=3\n","ChunkNumber=0\n","sample_number=sum([0])\n","use_know = True\n","set_use_know(use_know)\n","path = os.path.join(results_path,'kpfaoselm_gp_combiner_with_model_replacement_dynamic_feature_selection')\n","os.makedirs(path, exist_ok=True)\n","f_name,d_name = filenames[DATA_NUMBER],data_name[DATA_NUMBER]\n","DFS_results_path = os.path.join(feature_selection_results,d_name)\n","main(f_name, generate_oselm_models, apply_model_replacement=True,transfer_learning=True,\n","     feature_selection=[\"with_out_feature_evolving\",'DFS_feature_selection'], result_save_path=path,\n","     DFS_results_path=DFS_results_path,ChunkNumber=ChunkNumber,sample_number=sample_number)"],"metadata":{"id":"nNkXqHfi1NeX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DATA_NUMBER=4\n","ChunkNumber=0\n","sample_number=sum([0])\n","use_know = True\n","set_use_know(use_know)\n","path = os.path.join(results_path,'kpfaoselm_gp_combiner_with_model_replacement_dynamic_feature_selection')\n","os.makedirs(path, exist_ok=True)\n","f_name,d_name = filenames[DATA_NUMBER],data_name[DATA_NUMBER]\n","DFS_results_path = os.path.join(feature_selection_results,d_name)\n","main(f_name, generate_oselm_models, apply_model_replacement=True,transfer_learning=True,\n","     feature_selection=[\"with_out_feature_evolving\",'DFS_feature_selection'], result_save_path=path,\n","     DFS_results_path=DFS_results_path,ChunkNumber=ChunkNumber,sample_number=sample_number)"],"metadata":{"id":"nkfTHYRm1Ngv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DATA_NUMBER=5\n","ChunkNumber=0\n","sample_number=sum([0])\n","use_know = True\n","set_use_know(use_know)\n","path = os.path.join(results_path,'kpfaoselm_gp_combiner_with_model_replacement_dynamic_feature_selection')\n","os.makedirs(path, exist_ok=True)\n","f_name,d_name = filenames[DATA_NUMBER],data_name[DATA_NUMBER]\n","DFS_results_path = os.path.join(feature_selection_results,d_name)\n","main(f_name, generate_oselm_models, apply_model_replacement=True,transfer_learning=True,\n","     feature_selection=[\"with_out_feature_evolving\",'DFS_feature_selection'], result_save_path=path,\n","     DFS_results_path=DFS_results_path,ChunkNumber=ChunkNumber,sample_number=sample_number)"],"metadata":{"id":"ziOrD1W71NkP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DATA_NUMBER=6\n","ChunkNumber=0\n","sample_number=sum([0])\n","use_know = True\n","set_use_know(use_know)\n","path = os.path.join(results_path,'kpfaoselm_gp_combiner_with_model_replacement_dynamic_feature_selection')\n","os.makedirs(path, exist_ok=True)\n","f_name,d_name = filenames[DATA_NUMBER],data_name[DATA_NUMBER]\n","DFS_results_path = os.path.join(feature_selection_results,d_name)\n","main(f_name, generate_oselm_models, apply_model_replacement=True,transfer_learning=True,\n","     feature_selection=[\"with_out_feature_evolving\",'DFS_feature_selection'], result_save_path=path,\n","     DFS_results_path=DFS_results_path,ChunkNumber=ChunkNumber,sample_number=sample_number)"],"metadata":{"id":"HcjshOVA1Nns"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DATA_NUMBER=7\n","ChunkNumber=0\n","sample_number=sum([0])\n","use_know = True\n","set_use_know(use_know)\n","path = os.path.join(results_path,'kpfaoselm_gp_combiner_with_model_replacement_dynamic_feature_selection')\n","os.makedirs(path, exist_ok=True)\n","f_name,d_name = filenames[DATA_NUMBER],data_name[DATA_NUMBER]\n","DFS_results_path = os.path.join(feature_selection_results,d_name)\n","main(f_name, generate_oselm_models, apply_model_replacement=True,transfer_learning=True,\n","     feature_selection=[\"with_out_feature_evolving\",'DFS_feature_selection'], result_save_path=path,\n","     DFS_results_path=DFS_results_path,ChunkNumber=ChunkNumber,sample_number=sample_number)"],"metadata":{"id":"qkrY5dK91Nv5"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eX6-DgG5tj2C"},"outputs":[],"source":["DATA_NUMBER=8\n","ChunkNumber=0\n","sample_number=sum([0])\n","use_know = True\n","set_use_know(use_know)\n","path = os.path.join(results_path,'kpfaoselm_gp_combiner_with_model_replacement_dynamic_feature_selection')\n","os.makedirs(path, exist_ok=True)\n","f_name,d_name = filenames[DATA_NUMBER],data_name[DATA_NUMBER]\n","DFS_results_path = os.path.join(feature_selection_results,d_name)\n","main(f_name, generate_oselm_models, apply_model_replacement=True,transfer_learning=True,\n","     feature_selection=[\"with_out_feature_evolving\",'DFS_feature_selection'], result_save_path=path,\n","     DFS_results_path=DFS_results_path,ChunkNumber=ChunkNumber,sample_number=sample_number)"]},{"cell_type":"code","source":[],"metadata":{"id":"NjBNedoX1Xt7"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}