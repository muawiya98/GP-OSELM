{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPJ2A3qYo3EBk/nVV4U7sS/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XaPAjZM0Xjvr","executionInfo":{"status":"ok","timestamp":1695713555150,"user_tz":-180,"elapsed":32912,"user":{"displayName":"Muawiya Al danaf","userId":"16999440410141197833"}},"outputId":"fcab1367-5ec7-49e3-a49b-e67f57911dfa"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/450.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.5/450.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.6/450.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for scikit-multiflow (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q scikit-multiflow"]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from skmultiflow.drift_detection import ADWIN, DDM\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.preprocessing import OneHotEncoder\n","from matplotlib.colors import ListedColormap\n","from multiprocessing.pool import ThreadPool\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import roc_auc_score\n","from imblearn.over_sampling import SMOTE\n","from sklearn.utils import shuffle\n","from contextlib import suppress\n","from collections import Counter\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","from random import shuffle\n","import seaborn as sns\n","from time import time\n","import pandas as pd\n","import numpy as np\n","import warnings\n","import scipy.io\n","import pickle\n","import sys\n","import gc\n","import os\n","import re"],"metadata":{"id":"j6mOa_5zZcd7","executionInfo":{"status":"ok","timestamp":1695713564235,"user_tz":-180,"elapsed":3023,"user":{"displayName":"Muawiya Al danaf","userId":"16999440410141197833"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"65dn0vf7ZeUs","executionInfo":{"status":"ok","timestamp":1695713608476,"user_tz":-180,"elapsed":40625,"user":{"displayName":"Muawiya Al danaf","userId":"16999440410141197833"}},"outputId":"46498908-96d3-45b4-f79e-2882820ac7f0"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["data_path = '/content/drive/My Drive/Colab Notebooks/Muawiya/Genetic Programming Combiner with DFS/data'\n","code_path = '/content/drive/My Drive/Colab Notebooks/Muawiya/Genetic Programming Combiner with DFS/Codes/Shared Codes'\n","results_path = '/content/drive/My Drive/Colab Notebooks/Muawiya/Genetic Programming Combiner with DFS/Results'\n","feature_selection_results = '/content/drive/My Drive/Colab Notebooks/Muawiya/Genetic Programming Combiner with DFS/feature_selection_results'"],"metadata":{"id":"NNiPWlBkZgBj","executionInfo":{"status":"ok","timestamp":1695713634178,"user_tz":-180,"elapsed":3,"user":{"displayName":"Muawiya Al danaf","userId":"16999440410141197833"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def save_pickle(obj, file_name):\n","  with open(file_name, 'wb') as f:\n","    pickle.dump(obj, f)\n","def load_pickle(file_name):\n","  with open(file_name, 'rb') as f:\n","    d = pickle.load(f)\n","  return d"],"metadata":{"id":"Zls_3lUeaa6j","executionInfo":{"status":"ok","timestamp":1695713704402,"user_tz":-180,"elapsed":326,"user":{"displayName":"Muawiya Al danaf","userId":"16999440410141197833"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["sys.path.insert(0,code_path)\n","from oselm import set_use_know\n","from SharedFunctions import main,generate_oselm_models"],"metadata":{"id":"s4AxNS7bZiBa","executionInfo":{"status":"ok","timestamp":1695713650242,"user_tz":-180,"elapsed":4969,"user":{"displayName":"Muawiya Al danaf","userId":"16999440410141197833"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["filenames = ['kddcup99_csv.csv','ISCX2012.csv','CSE-CIC2018.csv','CICIDS2017.csv','7recurrentDrift.csv', 'blip.csv', 'incrementalDrift.csv',\n","             '7gradualDrift.csv', '7suddenDrift.csv']\n","filenames = list(map(lambda x: os.path.join(data_path, x), filenames))\n","data_name = ['kddcup99','ISCX2012','CSE-CIC2018','CICIDS2017','7recurrentDrift', 'blip', 'incrementalDrift','7gradualDrift', '7suddenDrift']"],"metadata":{"id":"e3gcoza8Xqni","executionInfo":{"status":"ok","timestamp":1695713651267,"user_tz":-180,"elapsed":2,"user":{"displayName":"Muawiya Al danaf","userId":"16999440410141197833"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def prepare_data(csv_filename, target_column_name='class'):\n","    # read csv file\n","    df = pd.read_csv(csv_filename)\n","    df = df.iloc[:80000, :]\n","    column_names = df.columns.tolist()\n","    if target_column_name not in column_names:\n","        target_column_name = column_names[-1]\n","    # get unique value in target column\n","    unique_vlaues = sorted(df[target_column_name].unique().tolist())\n","    df[target_column_name] = df[target_column_name].apply(lambda x: 0 if x == unique_vlaues[0] else 1)\n","    df[target_column_name] = df[target_column_name].astype('int')\n","    # rename the column of the dataframe\n","    num_of_columns = len(column_names)\n","    df.columns = list(range(num_of_columns))\n","    return df"],"metadata":{"id":"kdu7XH-uah_q","executionInfo":{"status":"ok","timestamp":1695713734481,"user_tz":-180,"elapsed":3,"user":{"displayName":"Muawiya Al danaf","userId":"16999440410141197833"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["for f_name,d_name in zip(filenames,data_name):\n","  results, datasets = {}, {}\n","  d = prepare_data(f_name)\n","  d = d.sample(frac=1, random_state=42)\n","  buffer = d.sample(n=5000)\n","  d.reset_index(inplace=True)\n","  d.replace([np.inf], 0, inplace=True)\n","  datasets[f_name.split('/')[-1]] = d\n","  ensemble = None\n","  drift_detection_obj = DDM()\n","  for key in tqdm(datasets.keys()):\n","      result_save_path_data = os.path.join(result_save_path, key)\n","      drift_location = {}\n","      prediction_times = {}\n","      memory_reduction = {}\n","\n","      results[key] = {'model_result': []}\n","      data = datasets[key].values\n","      X, Y = data[:, 0:-1], data[:, -1].astype('int')\n","      if not os.path.exists(\"{}_evolving_matrix.pkl\".format(os.path.join(evolving_path, key))):\n","        a2 = np.random.randint(low=0, high=X.shape[1], size = X.shape[1] // 6).tolist()\n","        a3 = np.random.randint(low=0, high=X.shape[1], size = X.shape[1] // 5).tolist()\n","        a4 = np.random.randint(low=0, high=X.shape[1], size = X.shape[1] // 4).tolist()\n","        evolving_matrix = [a2, a3, a4]\n","        save_pickle(evolving_matrix, \"{}_evolving_matrix.pkl\".format(os.path.join(evolving_path, key)))\n","      else:\n","        evolving_matrix = load_pickle(\"{}_evolving_matrix.pkl\".format(os.path.join(evolving_path, key)))\n","      ensemble = generate_model(number_of_hidden_neurons=X.shape[1]*3 // 2, apply_model_replacement=apply_model_replacement)\n","      chunks_features = np.array_split(X, 10)\n","      chunks_labels = np.array_split(Y, 10)\n","      print(\"===================== dataset : {} ======================\".format(key))\n","      chunk_number = 1\n","      for CN,chunk_X, chunk_Y in tqdm(zip([*range(len(chunks_labels))],chunks_features, chunks_labels)):\n","          drift = False\n","          try:\n","            chunk_X, chunk_Y = SMOTE().fit_resample(chunk_X, chunk_Y)\n","          except:\n","            if chunk_Y.sum() in [0, 1]:\n","              new_samples, new_labels = generate_new_samples(buffer, chunk_Y)\n","              chunk_X = np.concatenate((chunk_X, new_samples))\n","              chunk_Y = np.concatenate((chunk_Y, new_labels))\n","          gc.collect()\n","\n","          unselected_feautres = None\n","          selected = None\n","          Drift_Location_path = os.path.join(DriftLocation,key)\n","          if not os.path.exists(os.path.join(DriftLocation, \"drift_location\")):\n","            os.makedirs(Drift_Location_path, exist_ok=True)\n","            if chunk_number > 1:\n","              for i in tqdm(range(len(chunk_X))):\n","                if drift:\n","                  drift_location[chunk_number] = 'drift'\n","                  break\n","                x, y_true = chunk_X[i], chunk_Y[i]\n","                drift = E2SC4ID(x,y_true,ensemble=ensemble,drift_detection_obj=drift_detection_obj)\n","            save_object(drift_location, \"drift_location\", Drift_Location_path)\n","          else:\n","            save_object(drift_location, \"drift_location\", Drift_Location_path)\n","            drift = None"],"metadata":{"id":"t3xBcGpbXqqS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dXbcg-DjXqu6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EoHMH62qXqxa"},"execution_count":null,"outputs":[]}]}